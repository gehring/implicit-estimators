import experiments.chain.supervised.experiment
import experiments.chain.supervised.explicit

LEARNING_RATE = 1e-2
BATCH_SIZE = 1
DISCOUNT = 0.99
MDP_MODULE_DISCOUNT = 0.99

weights/ValueIteration.tol = 1e-2
weights/ValueIteration.maxiter = 1000
weights/ValueIteration.reduce = @max_reduce
weights/ValueIteration.offset = @identity_offset

weights/ExplicitMDP.num_pseudo_actions = 1
weights/ExplicitMDP.discount_init = %MDP_MODULE_DISCOUNT

MDPSolveWeights.solver = @weights/ValueIteration()
MDPSolveWeights.mdp_module = @weights/ExplicitMDP()

ExplicitWeights.implicit_module = @MDPSolveWeights()

LinearModule.weight_module = @ExplicitWeights()

dataset/ValueIteration.tol = 1e-3
dataset/ValueIteration.maxiter = 2000
dataset/ValueIteration.reduce = @max_reduce
dataset/ValueIteration.offset = @identity_offset

create_chain_mdp.num_states = 11
create_chain_mdp.slip_prob = 0.
create_chain_mdp.good_reward = 10.
create_chain_mdp.bad_reward = 1.
create_chain_mdp.discount = %DISCOUNT

supervised_chain_dataset.value_solver = @dataset/ValueIteration()

batch_generator.data = @supervised_chain_dataset()
batch_generator.batch_size = %BATCH_SIZE
batch_generator.replace = True

adam.learning_rate = %LEARNING_RATE

experiment.train.model = @LinearModule()
experiment.train.optimizer = @adam()
experiment.train.test_data = @supervised_chain_dataset()
experiment.train.num_iterations = 200
experiment.train.eval_period = 10

experiment.run_loop.seed = %SEED

run.target = @experiment.run_loop
launch.hyperparams = {
    "LEARNING_RATE": [0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125],
    "BATCH_SIZE": [1, 4, 10],
}

import chain.configurables

LEARNING_RATE = 1e-2
BATCH_SIZE = 1
DISCOUNT = 0.99

MDP_MODULE_DISCOUNT = %DISCOUNT
WEIGHTS_VI_TOL = 1e-4

weights/ValueIteration.tol = %WEIGHTS_VI_TOL
weights/ValueIteration.maxiter = 5000
weights/ValueIteration.reduce = @max_reduce
weights/ValueIteration.offset = @identity_offset

weights/ExplicitMDP.num_pseudo_actions = 1
weights/ExplicitMDP.discount_init = %MDP_MODULE_DISCOUNT

MDPSolveWeights.solver = @weights/ValueIteration()
MDPSolveWeights.mdp_module = @weights/ExplicitMDP()

LinearModule.weight_module = @MDPSolveWeights()

dataset/ValueIteration.tol = 1e-4
dataset/ValueIteration.maxiter = 10000
dataset/ValueIteration.reduce = @max_reduce
dataset/ValueIteration.offset = @identity_offset

create_chain_mdp.num_states = 31
create_chain_mdp.slip_prob = 0.
create_chain_mdp.good_reward = 10.
create_chain_mdp.bad_reward = 1.
create_chain_mdp.discount = %DISCOUNT

supervised_dataset.mdp = @create_chain_mdp()
supervised_dataset.value_solver = @dataset/ValueIteration()

batch_generator.data = @supervised_dataset()
batch_generator.batch_size = %BATCH_SIZE
batch_generator.replace = True

adam.learning_rate = %LEARNING_RATE
sgd.learning_rate = %LEARNING_RATE

train.model = @LinearModule()
#train.optimizer = @adam()
train.optimizer = @sgd()
train.test_data = @supervised_dataset()
train.num_iterations = 200
train.eval_period = 10
